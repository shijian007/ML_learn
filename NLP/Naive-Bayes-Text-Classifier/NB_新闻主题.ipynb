{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import jieba  #处理中文\n",
    "#import nltk  #处理英文\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#粗暴的词去重\n",
    "def make_word_set(words_file):\n",
    "    words_set = set()\n",
    "    with open(words_file, 'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            word = line.strip().decode(\"utf-8\")\n",
    "            if len(word)>0 and word not in words_set: # 去重\n",
    "                words_set.add(word)\n",
    "    return words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 文本处理，也就是样本生成过程\n",
    "def text_processing(folder_path, test_size=0.2):\n",
    "    folder_list = os.listdir(folder_path)\n",
    "    data_list = []\n",
    "    class_list = []\n",
    "\n",
    "    # 遍历文件夹\n",
    "    for folder in folder_list:\n",
    "        new_folder_path = os.path.join(folder_path, folder)\n",
    "        files = os.listdir(new_folder_path)\n",
    "        # 读取文件\n",
    "        j = 1\n",
    "        for file in files:\n",
    "            if j > 100: # 怕内存爆掉，只取100个样本文件，你可以注释掉取完\n",
    "                break\n",
    "            with open(os.path.join(new_folder_path, file), 'r') as fp:\n",
    "               raw = fp.read()\n",
    "            ## 是的，随处可见的jieba中文分词\n",
    "            jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数，不支持windows\n",
    "            word_cut = jieba.cut(raw, cut_all=False) # 精确模式，返回的结构是一个可迭代的genertor\n",
    "            word_list = list(word_cut) # genertor转化为list，每个词unicode格式\n",
    "            jieba.disable_parallel() # 关闭并行分词模式\n",
    "            \n",
    "            data_list.append(word_list) #训练集list\n",
    "            class_list.append(folder.decode('utf-8')) #类别\n",
    "            j += 1\n",
    "    \n",
    "    ## 粗暴地划分训练集和测试集\n",
    "    data_class_list = zip(data_list, class_list)\n",
    "    random.shuffle(data_class_list)\n",
    "    index = int(len(data_class_list)*test_size)+1\n",
    "    train_list = data_class_list[index:]\n",
    "    test_list = data_class_list[:index]\n",
    "    train_data_list, train_class_list = zip(*train_list)\n",
    "    test_data_list, test_class_list = zip(*test_list)\n",
    "    \n",
    "    #其实可以用sklearn自带的部分做\n",
    "    #train_data_list, test_data_list, train_class_list, test_class_list = sklearn.cross_validation.train_test_split(data_list, class_list, test_size=test_size)\n",
    "    \n",
    "\n",
    "    # 统计词频放入all_words_dict\n",
    "    all_words_dict = {}\n",
    "    for word_list in train_data_list:\n",
    "        for word in word_list:\n",
    "            if all_words_dict.has_key(word):\n",
    "                all_words_dict[word] += 1\n",
    "            else:\n",
    "                all_words_dict[word] = 1\n",
    "\n",
    "    # key函数利用词频进行降序排序\n",
    "    all_words_tuple_list = sorted(all_words_dict.items(), key=lambda f:f[1], reverse=True) # 内建函数sorted参数需为list\n",
    "    all_words_list = list(zip(*all_words_tuple_list)[0])\n",
    "\n",
    "    return all_words_list, train_data_list, test_data_list, train_class_list, test_class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words_dict(all_words_list, deleteN, stopwords_set=set()):\n",
    "    # 选取特征词\n",
    "    feature_words = []\n",
    "    n = 1\n",
    "    for t in range(deleteN, len(all_words_list), 1):\n",
    "        if n > 1000: # feature_words的维度1000\n",
    "            break\n",
    "            \n",
    "        if not all_words_list[t].isdigit() and all_words_list[t] not in stopwords_set and 1<len(all_words_list[t])<5:\n",
    "            feature_words.append(all_words_list[t])\n",
    "            n += 1\n",
    "    return feature_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 文本特征\n",
    "def text_features(train_data_list, test_data_list, feature_words, flag='nltk'):\n",
    "    def text_features(text, feature_words):\n",
    "        text_words = set(text)\n",
    "        ## -----------------------------------------------------------------------------------\n",
    "        if flag == 'nltk':\n",
    "            ## nltk特征 dict\n",
    "            features = {word:1 if word in text_words else 0 for word in feature_words}\n",
    "        elif flag == 'sklearn':\n",
    "            ## sklearn特征 list\n",
    "            features = [1 if word in text_words else 0 for word in feature_words]\n",
    "        else:\n",
    "            features = []\n",
    "        ## -----------------------------------------------------------------------------------\n",
    "        return features\n",
    "    train_feature_list = [text_features(text, feature_words) for text in train_data_list]\n",
    "    test_feature_list = [text_features(text, feature_words) for text in test_data_list]\n",
    "    return train_feature_list, test_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分类，同时输出准确率等\n",
    "def text_classifier(train_feature_list, test_feature_list, train_class_list, test_class_list, flag='nltk'):\n",
    "    ## -----------------------------------------------------------------------------------\n",
    "    if flag == 'nltk':\n",
    "        ## 使用nltk分类器\n",
    "        train_flist = zip(train_feature_list, train_class_list)\n",
    "        test_flist = zip(test_feature_list, test_class_list)\n",
    "        classifier = nltk.classify.NaiveBayesClassifier.train(train_flist)\n",
    "        test_accuracy = nltk.classify.accuracy(classifier, test_flist)\n",
    "    elif flag == 'sklearn':\n",
    "        ## sklearn分类器\n",
    "        classifier = MultinomialNB().fit(train_feature_list, train_class_list)\n",
    "        test_accuracy = classifier.score(test_feature_list, test_class_list)\n",
    "    else:\n",
    "        test_accuracy = []\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0x80 in position 2: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-59557e9eaf6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;31m## 文本预处理\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfolder_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./Database/SogouC/Sample'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mall_words_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_class_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_class_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;31m# 生成stopwords_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c5cbbe177e17>\u001b[0m in \u001b[0;36mtext_processing\u001b[0;34m(folder_path, test_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_folder_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                \u001b[0mraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[1;31m## 是的，随处可见的jieba中文分词\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_parallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 开启并行分词模式，参数为并行进程数，不支持windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0x80 in position 2: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "print(\"start\")\n",
    "\n",
    "## 文本预处理\n",
    "folder_path = './Database/SogouC/Sample'\n",
    "all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = text_processing(folder_path, test_size=0.2)\n",
    "\n",
    "# 生成stopwords_set\n",
    "stopwords_file = './stopwords_cn.txt'\n",
    "stopwords_set = make_word_set(stopwords_file)\n",
    "\n",
    "## 文本特征提取和分类\n",
    "# flag = 'nltk'\n",
    "flag = 'sklearn'\n",
    "deleteNs = range(0, 1000, 20)\n",
    "test_accuracy_list = []\n",
    "for deleteN in deleteNs:\n",
    "    # feature_words = words_dict(all_words_list, deleteN)\n",
    "    feature_words = words_dict(all_words_list, deleteN, stopwords_set)\n",
    "    train_feature_list, test_feature_list = text_features(train_data_list, test_data_list, feature_words, flag)\n",
    "    test_accuracy = text_classifier(train_feature_list, test_feature_list, train_class_list, test_class_list, flag)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "print(test_accuracy_list)\n",
    "\n",
    "# 结果评价\n",
    "#plt.figure()\n",
    "plt.plot(deleteNs, test_accuracy_list)\n",
    "plt.title('Relationship of deleteNs and test_accuracy')\n",
    "plt.xlabel('deleteNs')\n",
    "plt.ylabel('test_accuracy')\n",
    "plt.show()\n",
    "#plt.savefig('result.png')\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
