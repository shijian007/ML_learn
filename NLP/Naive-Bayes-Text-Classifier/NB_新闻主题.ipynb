{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiezhijun/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#coding: utf-8\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import jieba  #处理中文\n",
    "#import nltk  #处理英文\n",
    "import sklearn\n",
    "from sklearn import cross_validation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#粗暴的词去重\n",
    "def make_word_set(words_file):\n",
    "    words_set = set()\n",
    "    with open(words_file, 'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            word = line.strip().decode(\"utf-8\")\n",
    "            if len(word)>0 and word not in words_set: # 去重\n",
    "                words_set.add(word)\n",
    "    return words_set"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 41,
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#词去重\n",
    "def remove_repition(words_file):\n",
    "    word_set = set()\n",
    "    with open(words_file, 'r') as fp:\n",
    "        for lines in fp.readlines():\n",
    "            word = lines.strip().decode(\"utf-8\")\n",
    "            if word not in word_set and len(word) > 0:\n",
    "                word_set.add(word)\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
>>>>>>> b5ae091e5e56193cd2d814388a0f73be6d0b424e
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 文本处理，也就是样本生成过程\n",
    "def text_processing(folder_path, test_size=0.2):\n",
    "    folder_list = os.listdir(folder_path)\n",
    "    data_list = []\n",
    "    class_list = []\n",
    "\n",
    "    # 遍历文件夹\n",
    "    for folder in folder_list:\n",
    "        new_folder_path = os.path.join(folder_path, folder)\n",
    "        files = os.listdir(new_folder_path)\n",
    "        # 读取文件\n",
    "        j = 1\n",
    "        for file in files:\n",
    "            if j > 100: # 怕内存爆掉，只取100个样本文件，你可以注释掉取完\n",
    "                break\n",
    "            with open(os.path.join(new_folder_path, file), 'r',errors='ignore') as fp:\n",
    "               raw = fp.read()\n",
    "            ## 是的，随处可见的jieba中文分词\n",
    "            #jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数，不支持windows\n",
    "            word_cut = jieba.cut(raw, cut_all=False) # 精确模式，返回的结构是一个可迭代的genertor\n",
    "            word_list = list(word_cut) # genertor转化为list，每个词unicode格式\n",
    "            #jieba.disable_parallel() # 关闭并行分词模式\n",
    "            \n",
    "            data_list.append(word_list) #训练集list\n",
    "            class_list.append(folder.encode('utf-8')) #类别\n",
    "#             class_list.append(folder)\n",
    "            j += 1\n",
    "    \n",
    "    ## 粗暴地划分训练集和测试集\n",
    "    data_class_list = zip(data_list, class_list)\n",
    "    random.shuffle(data_class_list)\n",
    "    index = int(len(data_class_list)*test_size)+1\n",
    "    train_list = data_class_list[index:]\n",
    "    test_list = data_class_list[:index]\n",
    "#     train_data_list, train_class_list = zip(*train_list)\n",
    "#     test_data_list, test_class_list = zip(*test_list)\n",
    "    \n",
    "    #其实可以用sklearn自带的部分做\n",
    "    train_data_list, test_data_list, train_class_list, test_class_list = sklearn.cross_validation.train_test_split(data_list, class_list, test_size=test_size)\n",
    "    \n",
    "\n",
    "    # 统计词频放入all_words_dict\n",
    "    all_words_dict = {}\n",
    "    for word_list in train_data_list:\n",
    "        for word in word_list:\n",
    "            if word in all_words_dict:\n",
    "#             if all_words_dict.has_key(word):\n",
    "                all_words_dict[word] += 1\n",
    "            else:\n",
    "                all_words_dict[word] = 1\n",
    "\n",
    "    # key函数利用词频进行降序排序\n",
    "    all_words_tuple_list = sorted(all_words_dict.items(), key=lambda f:f[1], reverse=True) # 内建函数sorted参数需为list\n",
    "    all_words_list = list(zip(*all_words_tuple_list)[0])\n",
    "\n",
    "    return all_words_list, train_data_list, test_data_list, train_class_list, test_class_list"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 42,
=======
   "execution_count": 16,
>>>>>>> b5ae091e5e56193cd2d814388a0f73be6d0b424e
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words_dict(all_words_list, deleteN, stopwords_set=set()):\n",
    "    # 选取特征词\n",
    "    feature_words = []\n",
    "    n = 1\n",
    "    for t in range(deleteN, len(all_words_list), 1):\n",
    "        if n > 1000: # feature_words的维度1000\n",
    "            break\n",
    "            \n",
    "        if not all_words_list[t].isdigit() and all_words_list[t] not in stopwords_set and 1<len(all_words_list[t])<5:\n",
    "            feature_words.append(all_words_list[t])\n",
    "            n += 1\n",
    "    return feature_words"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 43,
=======
   "execution_count": 17,
>>>>>>> b5ae091e5e56193cd2d814388a0f73be6d0b424e
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 文本特征\n",
    "def text_features(train_data_list, test_data_list, feature_words, flag='nltk'):\n",
    "    def text_features(text, feature_words):\n",
    "        text_words = set(text)\n",
    "        ## -----------------------------------------------------------------------------------\n",
    "        if flag == 'nltk':\n",
    "            ## nltk特征 dict\n",
    "            features = {word:1 if word in text_words else 0 for word in feature_words}\n",
    "        elif flag == 'sklearn':\n",
    "            ## sklearn特征 list\n",
    "            features = [1 if word in text_words else 0 for word in feature_words]\n",
    "        else:\n",
    "            features = []\n",
    "        ## -----------------------------------------------------------------------------------\n",
    "        return features\n",
    "    train_feature_list = [text_features(text, feature_words) for text in train_data_list]\n",
    "    test_feature_list = [text_features(text, feature_words) for text in test_data_list]\n",
    "    return train_feature_list, test_feature_list"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 44,
=======
   "execution_count": 18,
>>>>>>> b5ae091e5e56193cd2d814388a0f73be6d0b424e
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分类，同时输出准确率等\n",
    "def text_classifier(train_feature_list, test_feature_list, train_class_list, test_class_list, flag='nltk'):\n",
    "    ## -----------------------------------------------------------------------------------\n",
    "    if flag == 'nltk':\n",
    "        ## 使用nltk分类器\n",
    "        train_flist = zip(train_feature_list, train_class_list)\n",
    "        test_flist = zip(test_feature_list, test_class_list)\n",
    "        classifier = nltk.classify.NaiveBayesClassifier.train(train_flist)\n",
    "        test_accuracy = nltk.classify.accuracy(classifier, test_flist)\n",
    "    elif flag == 'sklearn':\n",
    "        ## sklearn分类器\n",
    "        classifier = MultinomialNB().fit(train_feature_list, train_class_list)\n",
    "        test_accuracy = classifier.score(test_feature_list, test_class_list)\n",
    "    else:\n",
    "        test_accuracy = []\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 45,
   "metadata": {},
=======
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
>>>>>>> b5ae091e5e56193cd2d814388a0f73be6d0b424e
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\XIEZHI~1\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
<<<<<<< HEAD
     "ename": "TypeError",
     "evalue": "object of type 'zip' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-59557e9eaf6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## 文本预处理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./Database/SogouC/Sample'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mall_words_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_class_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_class_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 生成stopwords_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-b7fe24ee527f>\u001b[0m in \u001b[0;36mtext_processing\u001b[0;34m(folder_path, test_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m## 粗暴地划分训练集和测试集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdata_class_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_class_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_class_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtrain_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_class_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/random.py\u001b[0m in \u001b[0;36mshuffle\u001b[0;34m(self, x, random)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mrandbelow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m                 \u001b[0;31m# pick an element in x[:i+1] with which to exchange x[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'zip' has no len()"
=======
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.353 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-59557e9eaf6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;31m## 文本预处理\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfolder_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./Database/SogouC/Sample'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mall_words_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_class_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_class_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;31m# 生成stopwords_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-471b339ef403>\u001b[0m in \u001b[0;36mtext_processing\u001b[0;34m(folder_path, test_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mdata_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#训练集list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mclass_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#类别\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mj\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
>>>>>>> b5ae091e5e56193cd2d814388a0f73be6d0b424e
     ]
    }
   ],
   "source": [
    "print(\"start\")\n",
    "\n",
    "## 文本预处理\n",
    "folder_path = './Database/SogouC/Sample'\n",
    "all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = text_processing(folder_path, test_size=0.2)\n",
    "\n",
    "# 生成stopwords_set\n",
    "stopwords_file = './stopwords_cn.txt'\n",
    "stopwords_set = make_word_set(stopwords_file)\n",
    "\n",
    "## 文本特征提取和分类\n",
    "# flag = 'nltk'\n",
    "flag = 'sklearn'\n",
    "deleteNs = range(0, 1000, 20)\n",
    "test_accuracy_list = []\n",
    "for deleteN in deleteNs:\n",
    "    # feature_words = words_dict(all_words_list, deleteN)\n",
    "    feature_words = words_dict(all_words_list, deleteN, stopwords_set)\n",
    "    train_feature_list, test_feature_list = text_features(train_data_list, test_data_list, feature_words, flag)\n",
    "    test_accuracy = text_classifier(train_feature_list, test_feature_list, train_class_list, test_class_list, flag)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "print(test_accuracy_list)\n",
    "\n",
    "# 结果评价\n",
    "#plt.figure()\n",
    "plt.plot(deleteNs, test_accuracy_list)\n",
    "plt.title('Relationship of deleteNs and test_accuracy')\n",
    "plt.xlabel('deleteNs')\n",
    "plt.ylabel('test_accuracy')\n",
    "plt.show()\n",
    "#plt.savefig('result.png')\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
