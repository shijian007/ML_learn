{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "#import joblib\n",
    "%pylab inline\n",
    "plt.rcParams['figure.figsize'] = 8, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Intel MKL locally, but Atlas in production\n",
    "# from numpy.distutils.system_info import get_info\n",
    "# print(get_info('blas_opt'))\n",
    "# print(get_info('lapack_opt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is given for each store and we are tasked with predicting the department-wide sales for each store.\n",
    "\n",
    "**stores.csv**\n",
    "\n",
    "This file contains anonymized information about the 45 stores, indicating the type and size of store.\n",
    "\n",
    "\n",
    "**Train.csv**\n",
    "\n",
    "- Store - the store number\n",
    "- Dept - the department number\n",
    "- Date - the week\n",
    "- Weekly_Sales -  sales for the given department in the given store\n",
    "- IsHoliday - whether the week is a special holiday week\n",
    "\n",
    "**features.csv**\n",
    "\n",
    "- Store - the store number\n",
    "- Date - the week\n",
    "- Temperature - average temperature in the region\n",
    "- Fuel_Price - cost of fuel in the region\n",
    "- MarkDown1-5 - anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\n",
    "- CPI - the consumer price index\n",
    "- Unemployment - the unemployment rate\n",
    "- IsHoliday - whether the week is a special holiday week\n",
    "\n",
    "Looks like we'll have to join on store and date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data utilizing Pandas' .csv reader as it is significantly faster than NumPy's costly (due to intermediate python object steps) loadtxt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(421570, 5) (115064, 4) (8190, 12) (45, 3)\n"
     ]
    }
   ],
   "source": [
    "X_traindf = pd.read_table('train.csv', sep=',', warn_bad_lines=True, error_bad_lines=True)\n",
    "X_testdf = pd.read_table('test.csv', sep=',', warn_bad_lines=True, error_bad_lines=True) \n",
    "stores = pd.read_table('stores.csv', sep=',', warn_bad_lines=True, error_bad_lines=True) \n",
    "X_addtl_feat = pd.read_table('features.csv', sep=',', warn_bad_lines=True, error_bad_lines=True) \n",
    "\n",
    "print(X_traindf.shape, X_testdf.shape, X_addtl_feat.shape, stores.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Address missing store and departments between Train and Test? \n",
    "e.g. 99, stores 5,9,10 and 25 Perhaps mixed dummy creation addresses this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 420212, -1: 1285, 0: 73})\n"
     ]
    }
   ],
   "source": [
    "# Check Weekly_sales for negative and zero sales.\n",
    "\n",
    "from collections import Counter\n",
    "couter = Counter(np.sign(X_traindf['Weekly_Sales']).astype(int))\n",
    "print(couter)\n",
    "# print np.where(X_traindf.Weekly_Sales.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treat zeros in the `Weekly_Sales` column as missing since the probability of merchandise returns equalling merchandise purchases for any given week is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiezhijun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "for index in np.where(X_traindf.Weekly_Sales == 0)[0]:\n",
    "    X_traindf['Weekly_Sales'][index] = X_traindf.Weekly_Sales.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 420285, -1: 1285})\n"
     ]
    }
   ],
   "source": [
    "X_traindf['Weekly_Sales'].fillna(value = X_traindf.Weekly_Sales.median(),inplace = True)\n",
    "\n",
    "print(Counter(np.sign(X_traindf['Weekly_Sales']).astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace NaN values in the `MarkDown` columns with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 4158, 1: 4028, -1: 4})\n"
     ]
    }
   ],
   "source": [
    "#X_addtl_feat = X_addtl_feat.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "for column in ('MarkDown%s' % i for i in range(1,6)):\n",
    "    X_addtl_feat[column].fillna(0, inplace=True)\n",
    "    \n",
    "print(Counter(np.sign(X_addtl_feat['MarkDown1']).astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill `CPI` and `Unemployment` with column median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_addtl_feat['CPI'].fillna(value = X_addtl_feat.CPI.median(),inplace = True)\n",
    "X_addtl_feat['Unemployment'].fillna(value = X_addtl_feat.Unemployment.median(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Joining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store and Date will be safe to join on as the formatting looks to be consistent. If the data were large it would have necessitated making the analysis I/O bound rather than memory and we would have loaded in in a database or used a chunking method with HDF5. Our simple left join would have looked something like this in SQL:\n",
    "\n",
    "```SQL\n",
    "Select *\n",
    "FROM Train_Xdf A\n",
    "LEFT JOIN addtl_features B\n",
    "ON A.Store = B.Store\n",
    "AND A.Date = B.Date\n",
    "```\n",
    "\n",
    "Join the *features* and *stores* data set with the *train* and *test* datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>IsHoliday_x</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>IsHoliday_y</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-02</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-09</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-16</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-23</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  Dept        Date  IsHoliday_x  Temperature  Fuel_Price  MarkDown1  \\\n",
       "0      1     1  2012-11-02        False          NaN         NaN        NaN   \n",
       "1      1     1  2012-11-09        False          NaN         NaN        NaN   \n",
       "2      1     1  2012-11-16        False          NaN         NaN        NaN   \n",
       "3      1     1  2012-11-23         True          NaN         NaN        NaN   \n",
       "4      1     1  2012-11-30        False          NaN         NaN        NaN   \n",
       "\n",
       "   MarkDown2  MarkDown3  MarkDown4  MarkDown5  CPI  Unemployment IsHoliday_y  \\\n",
       "0        NaN        NaN        NaN        NaN  NaN           NaN         NaN   \n",
       "1        NaN        NaN        NaN        NaN  NaN           NaN         NaN   \n",
       "2        NaN        NaN        NaN        NaN  NaN           NaN         NaN   \n",
       "3        NaN        NaN        NaN        NaN  NaN           NaN         NaN   \n",
       "4        NaN        NaN        NaN        NaN  NaN           NaN         NaN   \n",
       "\n",
       "  Type    Size  \n",
       "0    A  151315  \n",
       "1    A  151315  \n",
       "2    A  151315  \n",
       "3    A  151315  \n",
       "4    A  151315  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_traindf = pd.merge(X_traindf, X_addtl_feat, how='left', on=['Store', 'Date'])\n",
    "X_traindf = pd.merge(X_traindf, stores, how='left', on='Store')\n",
    "\n",
    "X_testdf = pd.merge(X_testdf, X_addtl_feat, how='left', on=['Store', 'Date'])\n",
    "X_testdf = pd.merge(X_testdf, stores, how='left', on='Store')\n",
    "X_testdf.head()\n",
    "# X_traindf.sort(['Store', 'Date'], ascending=[1, 2]).head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Encoding\n",
    "\n",
    "_Markdown_ columns are not a result of binary encoding. The attached description does not clarify the values, but we'll have to assume it's a real scalar and not a categorical encoding based on all the unique values.\n",
    "\n",
    "The _IsHoliday_  and _Type_ columns are categorical and need to be binary-encoded, since we haven't got to NumPy yet we can utilize Pandas' *get_dummies* method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date**\n",
    "\n",
    "The Date column can be binarily encoded to create the following features:\n",
    "\n",
    "- <s>Hour of the day (24 boolean features)</s><br>\n",
    "- <s>Day of the week (7 boolean features)</s> <font color = 'red'> Only provided the first day of every week</font><br>\n",
    "- <s>Day of the month (up to 31 boolean features)</s><font color = 'red'> Ideal to specify 1-4 for week of month</font><br>\n",
    "- Month of the year (12 boolean features)<br>\n",
    "- Year (as many boolean features as they are different years in your dataset)<br>\n",
    "\n",
    "These features will enable us to identify linear dependencies on periodic events on typical time cycles. We can also create one contiunous feature derived from POSIX time.\n",
    "\n",
    "<br>\n",
    "<font color = \"dodgerblue\">Concatenate `traindf` and `testdf` before creating dummy features incase some stores/dates/departments don't exist in each others set. Split by index or 0 sales value later:</font>\n",
    "\n",
    "http://nbviewer.ipython.org/github/herrfz/dataanalysis/blob/master/assignment2/samsung_data_prediction_submitted.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiezhijun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday_x</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>IsHoliday_y</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-02</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-09</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-16</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-23</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-30</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  Dept        Date  Weekly_Sales  IsHoliday_x  Temperature  \\\n",
       "0      1     1  2012-11-02             0        False          NaN   \n",
       "1      1     1  2012-11-09             0        False          NaN   \n",
       "2      1     1  2012-11-16             0        False          NaN   \n",
       "3      1     1  2012-11-23             0         True          NaN   \n",
       "4      1     1  2012-11-30             0        False          NaN   \n",
       "\n",
       "   Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5  CPI  \\\n",
       "0         NaN        NaN        NaN        NaN        NaN        NaN  NaN   \n",
       "1         NaN        NaN        NaN        NaN        NaN        NaN  NaN   \n",
       "2         NaN        NaN        NaN        NaN        NaN        NaN  NaN   \n",
       "3         NaN        NaN        NaN        NaN        NaN        NaN  NaN   \n",
       "4         NaN        NaN        NaN        NaN        NaN        NaN  NaN   \n",
       "\n",
       "   Unemployment IsHoliday_y Type    Size  \n",
       "0           NaN         NaN    A  151315  \n",
       "1           NaN         NaN    A  151315  \n",
       "2           NaN         NaN    A  151315  \n",
       "3           NaN         NaN    A  151315  \n",
       "4           NaN         NaN    A  151315  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_testdf[\"Weekly_Sales\"] = 0\n",
    "\n",
    "# Reorder testdf to match 'Weekly Sales' column position of traindf. \n",
    "# A better way to do this would be to pd.concat reordered slices of test together\n",
    "X_testdf = X_testdf.ix[:, ['Store','Dept','Date','Weekly_Sales','IsHoliday_x','Temperature',\n",
    "                 'Fuel_Price','MarkDown1','MarkDown2','MarkDown3',\n",
    "                 'MarkDown4','MarkDown5','CPI','Unemployment','IsHoliday_y',\n",
    "                 'Type','Size']]\n",
    "X_testdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dfDummies_concat(dfx): \n",
    "    ''' Create and concatenate named non-default dummy variables for identified columns'''\n",
    "    \n",
    "    holidayDummies = pd.get_dummies(dfx['IsHoliday_x'])\n",
    "    holidayDummies.columns = ['IsHolidayF','IsHolidayT']\n",
    "    dfx.drop('IsHoliday_y', 1, inplace=True)\n",
    "    dfx.drop('IsHoliday_x', 1, inplace=True)\n",
    "    \n",
    "    dfx['Type'] = 'Type_' + dfx['Type'].astype(str)\n",
    "    typeDummies = pd.get_dummies(dfx['Type'])\n",
    "    typeDummies.columns = ['TypeA','TypeB', 'TypeC']\n",
    "    dfx.drop('Type', 1, inplace=True)\n",
    "    \n",
    "    dfx['Store'] = 'Store_' + dfx['Store'].astype(str)\n",
    "    storeDummies = pd.get_dummies(dfx['Store'])\n",
    "    dfx.drop('Store', 1, inplace=True)\n",
    "    \n",
    "    dfx['Dept'] = 'dept_' + dfx['Dept'].astype(str)\n",
    "    deptDummies = pd.get_dummies(dfx['Dept'])\n",
    "    dfx.drop('Dept', 1, inplace=True)\n",
    "\n",
    "    # There HAS to be a faster way than this lambda f(x)\n",
    "    dateSplit = dfx['Date'].apply(lambda x: pd.Series(x.split('-')))\n",
    "    dateSplit.columns  = ['year','month','day']\n",
    "    dateSplit['year']  = 'year_' + dateSplit['year'].astype(str)\n",
    "    dateSplit['month'] = 'month_' + dateSplit['month'].astype(str)\n",
    "    dateSplit['day']   = 'day_' + dateSplit['day'].astype(str)\n",
    "    \n",
    "    dfx.drop('Date',1,inplace=True)\n",
    "    \n",
    "\n",
    "    yearDummies  = pd.get_dummies(dateSplit['year'])\n",
    "    monthDummies = pd.get_dummies(dateSplit['month'])\n",
    "    dayDummies   = pd.get_dummies(dateSplit['day'])\n",
    "\n",
    "  \n",
    "    df_concat = pd.concat( [dfx, holidayDummies, typeDummies, \n",
    "                            storeDummies, deptDummies, yearDummies,\n",
    "                            monthDummies, dayDummies],\n",
    "                          join='outer', axis=1, ignore_index=False)\n",
    "\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday_x</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>IsHoliday_y</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>46039.49</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>41595.55</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>19403.54</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>21827.90</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  Dept        Date  Weekly_Sales  IsHoliday_x  Temperature  \\\n",
       "0      1     1  2010-02-05      24924.50        False          NaN   \n",
       "1      1     1  2010-02-12      46039.49         True          NaN   \n",
       "2      1     1  2010-02-19      41595.55        False          NaN   \n",
       "3      1     1  2010-02-26      19403.54        False          NaN   \n",
       "4      1     1  2010-03-05      21827.90        False          NaN   \n",
       "\n",
       "   Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5  CPI  \\\n",
       "0         NaN        NaN        NaN        NaN        NaN        NaN  NaN   \n",
       "1         NaN        NaN        NaN        NaN        NaN        NaN  NaN   \n",
       "2         NaN        NaN        NaN        NaN        NaN        NaN  NaN   \n",
       "3         NaN        NaN        NaN        NaN        NaN        NaN  NaN   \n",
       "4         NaN        NaN        NaN        NaN        NaN        NaN  NaN   \n",
       "\n",
       "   Unemployment IsHoliday_y Type    Size  \n",
       "0           NaN         NaN    A  151315  \n",
       "1           NaN         NaN    A  151315  \n",
       "2           NaN         NaN    A  151315  \n",
       "3           NaN         NaN    A  151315  \n",
       "4           NaN         NaN    A  151315  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfConcat = pd.concat( [X_traindf, X_testdf], join = 'outer', axis=0 , ignore_index = False )\n",
    "dfConcat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(536634, 189)\n"
     ]
    }
   ],
   "source": [
    "# %load_ext\n",
    "# %lprun\n",
    "dfConcat = dfDummies_concat(dfConcat)\n",
    "print(dfConcat.shape)\n",
    "\n",
    "# X_traindf = dfDummies_concat(X_traindf)\n",
    "# X_testdf = dfDummies_concat(X_testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>...</th>\n",
       "      <th>day_22</th>\n",
       "      <th>day_23</th>\n",
       "      <th>day_24</th>\n",
       "      <th>day_25</th>\n",
       "      <th>day_26</th>\n",
       "      <th>day_27</th>\n",
       "      <th>day_28</th>\n",
       "      <th>day_29</th>\n",
       "      <th>day_30</th>\n",
       "      <th>day_31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24924.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46039.49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41595.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19403.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21827.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 189 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Weekly_Sales  Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  \\\n",
       "0      24924.50          NaN         NaN        NaN        NaN        NaN   \n",
       "1      46039.49          NaN         NaN        NaN        NaN        NaN   \n",
       "2      41595.55          NaN         NaN        NaN        NaN        NaN   \n",
       "3      19403.54          NaN         NaN        NaN        NaN        NaN   \n",
       "4      21827.90          NaN         NaN        NaN        NaN        NaN   \n",
       "\n",
       "   MarkDown4  MarkDown5  CPI  Unemployment   ...    day_22  day_23  day_24  \\\n",
       "0        NaN        NaN  NaN           NaN   ...         0       0       0   \n",
       "1        NaN        NaN  NaN           NaN   ...         0       0       0   \n",
       "2        NaN        NaN  NaN           NaN   ...         0       0       0   \n",
       "3        NaN        NaN  NaN           NaN   ...         0       0       0   \n",
       "4        NaN        NaN  NaN           NaN   ...         0       0       0   \n",
       "\n",
       "   day_25  day_26  day_27  day_28  day_29  day_30  day_31  \n",
       "0       0       0       0       0       0       0       0  \n",
       "1       0       0       0       0       0       0       0  \n",
       "2       0       0       0       0       0       0       0  \n",
       "3       0       1       0       0       0       0       0  \n",
       "4       0       0       0       0       0       0       0  \n",
       "\n",
       "[5 rows x 189 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfConcat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dfConcat back into X_traindf and X_testdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(421570, 189) (115064, 189)\n"
     ]
    }
   ],
   "source": [
    "X_traindf = dfConcat[:421570]  \n",
    "X_testdf = dfConcat[421570:]  \n",
    "\n",
    "print(X_traindf.shape, X_testdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create X and y NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape:(421570, 188)\n",
      "y Shape:(421570,)\n",
      "toPredict Shape: (115064, 188)\n"
     ]
    }
   ],
   "source": [
    "X = X_traindf.values[:,1:]\n",
    "y = X_traindf['Weekly_Sales'].values\n",
    "\n",
    "toPredict = X_testdf.values[:,1:] # drop empty sales col\n",
    "\n",
    "print('X Shape:' + str(X.shape)) \n",
    "print('y Shape:' + str(y.shape))\n",
    "print('toPredict Shape: ' + str(toPredict.shape)) \n",
    "\n",
    "# import joblib\n",
    "# joblib.dump(X, 'X.pkl', compress=0, cache_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cross Validation splits\n",
    "Create train and test data splits for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape: (316177, 188)\n",
      "y_train Shape: (316177,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "print('X_train Shape: ' + str(X_train.shape)) \n",
    "print('y_train Shape: ' + str(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Scaling\n",
    "Address zeros and negatives in `Weekly_Sales` then center all non-dummy features to the mean and component wise scale to unit variance. <font color = \"red\">Do not scale dummies! </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 315219, -1: 958})\n"
     ]
    }
   ],
   "source": [
    "# BEFORE\n",
    "# np.bincount(np.sign(y_train).astype(int))\n",
    "from collections import Counter\n",
    "print(Counter(np.sign(y_train).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-184ceb084de2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be the same as scaling before creating the train and test splits since a fit on X_train and y_train are being used to transform other splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-e4f929db2eb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;31m# XScaler = StandardScaler().fit(X[:,0:10]) # Try fitting og X instead X_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mXTrainScaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0myScaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;31m# toPredictScaler = StandardScaler().fit(toPredict[:,0:10])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\xiezhijun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\xiezhijun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    578\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m    579\u001b[0m                         \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\xiezhijun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    396\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    397\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\xiezhijun\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     52\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     53\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 54\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# XScaler = StandardScaler().fit(X[:,0:10]) # Try fitting og X instead X_train\n",
    "XTrainScaler = StandardScaler().fit(X_train[:,0:10]) \n",
    "yScaler = StandardScaler().fit(y_train) \n",
    "# toPredictScaler = StandardScaler().fit(toPredict[:,0:10]) \n",
    "\n",
    "# Scale Train w/ X_Train\n",
    "\n",
    "X_train[:, 0:10] = XTrainScaler.transform(X_train[:,0:10])\n",
    "# y_train = yScaler.transform(y_train)\n",
    "# y_train = np.log(y_train + 1 - y_train.min())\n",
    "y_train =  log1p(y_train- y_train.min()) \n",
    "\n",
    "# Scale Test w/ X_Train\n",
    "\n",
    "X_test[:,0:10] = XTrainScaler.transform(X_test[:,0:10])\n",
    "# y_test = yScaler.transform(y_test)\n",
    "# y_test =  np.log(y_test + 1 - y_test.min())  # Should it be y_trains min and not y_test min? Not the case with X.\n",
    "y_test =  log1p(y_test - y_test.min()) \n",
    "\n",
    "# toPredict does not come from the same X as X_train & X_test do, is it logical to transform with XTrainScaler?\n",
    "# toPredict[:,0:10 ]= XTrainScaler.transform(toPredict[:,0:10])\n",
    "# toPredict[:,0:10 ]= XScaler.transform(toPredict[:,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 316176, 0: 1})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AFTER\n",
    "Counter(np.sign(y_train).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([300132]),),\n",
       " array([  9.57061353,   9.72214012,   8.56073724,  10.11692212,\n",
       "         11.22121194,  10.52838614,  10.8235135 ,   8.55351355,\n",
       "         11.22151846,   9.35209685,   9.3580377 ,   9.88105583,\n",
       "          9.48319405,   8.60317637,   8.52415689,   8.58802624,\n",
       "          8.54164187,   9.85464914,   9.41182955,   9.45255059,\n",
       "          9.05563554,   8.86294651,  10.65869926,   8.51577419,\n",
       "          8.51616066,   9.17038363,  11.05870274,   9.21214175,\n",
       "         10.5360383 ,   9.50304207,  11.080059  ,   9.43457293,\n",
       "          0.        ,   9.57213429,   9.61281769,   9.37953194,\n",
       "         10.10675977,   9.79129822,   8.56249155,   9.74193169,\n",
       "          9.29717382,  11.13505636,   9.20380808,   8.8645333 ,\n",
       "         10.08356075,   8.81884326,   8.88780181,   9.2350603 ,\n",
       "          8.85075262,  10.20708456]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y_train==0), y_train[300100:300150 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Random Forests\n",
    "<font color=\"red\">Severe bottleneck!</font>\n",
    "Olivier Grisel's joblib is a nice sklearn drop-in as RandomForest is trivially parallelizable since each processor can generate forests independently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# rf = RandomForestRegressor(n_jobs= -1)\n",
    "\n",
    "# # (n_estimators=10, criterion='mse', max_depth=None,\n",
    "# #                            min_samples_split=2, min_samples_leaf=1, max_features='auto',\n",
    "# #                            bootstrap=True, oob_score=False, n_jobs=-1, random_state=None,\n",
    "# #                            verbose=0, min_density=None, compute_importances=None)\n",
    "\n",
    "# rf.fit(X_train,y_train)\n",
    "\n",
    "# import joblib\n",
    "# joblib.dump(rf, 'rf_pickle/rf.pkl', compress=0, cache_size=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD\n",
    "\n",
    "Well start with a lineal model such as SGDRegressor. This regressor attempts to find a hyperplane that minimizes a certain loss function (the sum of squared distances from each instance to the hyperplane) using Stochastic Gradient Descent to find the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.955209</td>\n",
       "      <td> 1.489699</td>\n",
       "      <td> 0.470602</td>\n",
       "      <td> 0.661288</td>\n",
       "      <td>-0.083006</td>\n",
       "      <td>-0.075413</td>\n",
       "      <td>-0.146441</td>\n",
       "      <td> 1.100887</td>\n",
       "      <td>-0.442331</td>\n",
       "      <td>-0.139616</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.955209  1.489699  0.470602  0.661288 -0.083006 -0.075413 -0.146441   \n",
       "\n",
       "         7         8         9   10  11  12  13  14  15  16  17  18  19      \n",
       "0  1.100887 -0.442331 -0.139616   1   0   0   1   0   0   0   0   0   0 ...  \n",
       "\n",
       "[1 rows x 188 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check is features have been scaled appropriately\n",
    "pd.DataFrame(X_train[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation set is used for model selection, the test set for final model (the model which was selected by selection process) prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def trainEval(clf, X_train, y_train):    \n",
    "    clf.fit(X_train, y_train)\n",
    "    print \"Coefficient of determination on training set:\", clf.score(X_train, y_train)\n",
    "    scores = cross_val_score(clf, X_train, y_train, n_jobs =-1) #cv=cv\n",
    "    print \"Average coefficient of determination using 3-fold crossvalidation:\",np.mean(scores)\n",
    "    test_scores = cross_val_score(clf, X_test, y_test, n_jobs =-1) #cv=cv\n",
    "    print \"Score clf on X_test/y_test:\", np.mean(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.769838314209\n",
      "Average coefficient of determination using 3-fold crossvalidation: 0.768144896161\n",
      "Score clf on X_test/y_test: 0.748322735946\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf_sgd = linear_model.SGDRegressor(verbose=0,loss='squared_loss', penalty=None, shuffle=True, random_state=None)\n",
    "trainEval(clf_sgd,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.769050788749\n",
      "Average coefficient of determination using 3-fold crossvalidation: 0.767874629816\n",
      "Score clf on X_test/y_test: 0.748130106685\n"
     ]
    }
   ],
   "source": [
    "clf_sgd1 = linear_model.SGDRegressor(loss='squared_loss', penalty='l2', shuffle=True, random_state=None)\n",
    "trainEval(clf_sgd1,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.769124036479\n",
      "Average coefficient of determination using 3-fold crossvalidation: 0.767565505272\n",
      "Score clf on X_test/y_test: 0.747832060756\n"
     ]
    }
   ],
   "source": [
    "clf_sgd2 = linear_model.SGDRegressor(loss='squared_loss', penalty='l1', shuffle=True, random_state=None)\n",
    "trainEval(clf_sgd2,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.769313174026\n",
      "Average coefficient of determination using 3-fold crossvalidation: 0.767367899591\n",
      "Score clf on X_test/y_test: 0.747641425828\n"
     ]
    }
   ],
   "source": [
    "clf_sgd3 = linear_model.SGDRegressor(loss='squared_loss', penalty='elasticnet', shuffle=True, random_state=None)\n",
    "trainEval(clf_sgd3,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.772695558119\n",
      "Average coefficient of determination using 3-fold crossvalidation: 0.772388495731\n",
      "Score clf on X_test/y_test: 0.765669101638\n"
     ]
    }
   ],
   "source": [
    "clf_ridge = linear_model.Ridge()\n",
    "trainEval(clf_ridge,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using X_scaler:\n",
    "clf_ridge: 0.665545556463\n",
    "\n",
    "Using y_train for both y log transforms:\n",
    "clf_ridge: 0.752506048458 \n",
    "\n",
    "Using y_train and y_test for respective log transforms:\n",
    "clf_ridge:  0.768398420012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.763939276859\n",
      "Average coefficient of determination using 3-fold crossvalidation: 0.73263523501\n",
      "Score clf on X_test/y_test: 0.76200253459\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import svm\n",
    "# clf_svr = svm.SVR()\n",
    "# trainEval(clf_svr,X_train[0:20000],y_train[0:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from sklearn import ensemble\n",
    "# clf_et=ensemble.ExtraTreesRegressor(n_estimators=10, n_jobs = -1)\n",
    "# trainEval(clf_et,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print sort(zip(clf_et.feature_importances_,list(X_traindf.columns)),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from scipy.stats import randint as sp_randint\n",
    "# from time import time\n",
    "# from operator import itemgetter\n",
    "# from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "# # Util function to report best scores\n",
    "# def grscoreReport(grid_scores, n_top=3):\n",
    "#     top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "#     for i, score in enumerate(top_scores):\n",
    "#         print(\"Model with rank: {0}\".format(i + 1))\n",
    "#         print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "#               score.mean_validation_score,\n",
    "#               np.std(score.cv_validation_scores)))\n",
    "#         print(\"Parameters: {0}\".format(score.parameters))\n",
    "#         print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # specify parameters and distributions to sample from\n",
    "# param_dist = {\"max_depth\": [3, None],\n",
    "#               \"max_features\": [sp_randint(1, 11),'auto'],\n",
    "#               \"min_samples_split\": sp_randint(1, 11),\n",
    "#               \"min_samples_leaf\": sp_randint(1, 11),\n",
    "#               \"bootstrap\": [True, False],\n",
    "#               \"criterion\": ['mse']}\n",
    "\n",
    "# # run randomized search\n",
    "# start = time()\n",
    "\n",
    "# n_iter_search = 10\n",
    "\n",
    "# random_search = RandomizedSearchCV(rf, param_distributions=param_dist, \n",
    "#                                    n_iter=n_iter_search, n_jobs = -1)  \n",
    "\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"RandomizedSearchCV took %.2f minutes for %d candidates\"\n",
    "#       \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "\n",
    "# grscoreReport(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.dummy import DummyRegressor \n",
    "# reg = DummyRegressor()\n",
    "# reg.fit(X_train, y_train)\n",
    "# print reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to drop the 'Weekly Sales' zeros column from X_testdf before training for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115064, 4)\n",
      "With sales columns: (421570, 189)(115064, 189)\n",
      "Without: (316177, 188)(105393, 188)\n",
      "toPredict (test.csv): (115064, 188)\n"
     ]
    }
   ],
   "source": [
    "test_csv = pd.read_table('data/test.csv', sep=',', warn_bad_lines=True, error_bad_lines=True) \n",
    "\n",
    "print test_csv.shape \n",
    "print 'With sales columns: ' + str(X_traindf.shape) + str(X_testdf.shape)\n",
    "print 'Without: ' + str(X_train.shape) + str(X_test.shape)\n",
    "print 'toPredict (test.csv): ' + str(toPredict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.19 s, sys: 15.8 ms, total: 1.2 s\n",
      "Wall time: 834 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def CurrHr():\n",
    "    epoch_time = int(time.time())\n",
    "    epoch_time = epoch_time # Round to hour\n",
    "    current_hour = datetime.datetime.fromtimestamp(epoch_time).strftime('%Y%m%d%H%M%S')\n",
    "    return current_hour\n",
    "\n",
    "\n",
    "subResults = pd.DataFrame(clf_ridge.predict(toPredict), columns = ['Weekly_Sales'])\n",
    "# subResults =  (exp(subResults)-1)\n",
    "\n",
    "subLabels = pd.DataFrame(test_csv.Store.astype(str) + '_' + \n",
    "             test_csv.Dept.astype(str) + '_' +\n",
    "             test_csv.Date.astype(str),columns = ['Id'])\n",
    "subResults.shape , subLabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Weekly_Sales\n",
      "0      0.813753\n",
      "1      1.227697\n",
      "2      0.625241\n",
      "3      0.970099\n",
      "4      0.819807\n",
      "\n",
      "[5 rows x 1 columns]\n",
      "   Weekly_Sales\n",
      "0      0.595398\n",
      "1      0.800968\n",
      "2      0.485656\n",
      "3      0.678084\n",
      "4      0.598730\n",
      "\n",
      "[5 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print (exp(subResults)-1)[:5]\n",
    "print subResults[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dropping column names for unknown reason on concat\n",
    "dfForCSV = pd.concat([subLabels,subResults], axis=1, ignore_index=True,)\n",
    "dfForCSV.columns = ['Id', 'Weekly_Sales']\n",
    "dfForCSV.to_csv(CurrHr() + '.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from ggplot import *\n",
    "# ggplot(diamonds, aes('carat', 'price')) + \\\n",
    "# geom_point(alpha=1/20.) + \\\n",
    "# ylim(0, 20000)\n",
    "\n",
    "\n",
    "# ggplot(X_traindf, aes(x=Month, y=Monthly_Sales)) + geom_bar(stat=\"identity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
