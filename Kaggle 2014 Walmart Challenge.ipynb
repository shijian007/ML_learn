{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "%pylab inline\n",
    "plt.rcParams['figure.figsize'] = 8, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Intel MKL locally, but Atlas in production\n",
    "# from numpy.distutils.system_info import get_info\n",
    "# print(get_info('blas_opt'))\n",
    "# print(get_info('lapack_opt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is given for each store and we are tasked with predicting the department-wide sales for each store.\n",
    "\n",
    "**stores.csv**\n",
    "\n",
    "This file contains anonymized information about the 45 stores, indicating the type and size of store.\n",
    "\n",
    "\n",
    "**Train.csv**\n",
    "\n",
    "- Store - the store number\n",
    "- Dept - the department number\n",
    "- Date - the week\n",
    "- Weekly_Sales -  sales for the given department in the given store\n",
    "- IsHoliday - whether the week is a special holiday week\n",
    "\n",
    "**features.csv**\n",
    "\n",
    "- Store - the store number\n",
    "- Date - the week\n",
    "- Temperature - average temperature in the region\n",
    "- Fuel_Price - cost of fuel in the region\n",
    "- MarkDown1-5 - anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\n",
    "- CPI - the consumer price index\n",
    "- Unemployment - the unemployment rate\n",
    "- IsHoliday - whether the week is a special holiday week\n",
    "\n",
    "Looks like we'll have to join on store and date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data utilizing Pandas' .csv reader as it is significantly faster than NumPy's costly (due to intermediate python object steps) loadtxt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(421570, 5) (115064, 4) (8190, 12) (45, 3)\n"
     ]
    }
   ],
   "source": [
    "X_traindf = pd.read_table('data/train.csv', sep=',', warn_bad_lines=True, error_bad_lines=True)\n",
    "X_testdf = pd.read_table('data/test.csv', sep=',', warn_bad_lines=True, error_bad_lines=True) \n",
    "stores = pd.read_table('data/stores.csv', sep=',', warn_bad_lines=True, error_bad_lines=True) \n",
    "X_addtl_feat = pd.read_table('data/features.csv', sep=',', warn_bad_lines=True, error_bad_lines=True) \n",
    "\n",
    "print X_traindf.shape, X_testdf.shape, X_addtl_feat.shape, stores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Address missing store and departments between Train and Test? \n",
    "e.g. 99, stores 5,9,10 and 25 Perhaps mixed dummy creation addresses this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 420212, -1: 1285, 0: 73})\n"
     ]
    }
   ],
   "source": [
    "# Check Weekly_sales for negative and zero sales.\n",
    "\n",
    "from collections import Counter\n",
    "print Counter(np.sign(X_traindf['Weekly_Sales']).astype(int))\n",
    "# print np.where(X_traindf.Weekly_Sales.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treat zeros in the `Weekly_Sales` column as missing since the probability of merchandise returns equalling merchandise purchases for any given week is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index in np.where(X_traindf.Weekly_Sales == 0)[0]:\n",
    "    X_traindf['Weekly_Sales'][index] = X_traindf.Weekly_Sales.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_traindf['Weekly_Sales'].fillna(value = X_traindf.Weekly_Sales.median(),inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace NaN values in the `MarkDown` columns with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_addtl_feat = X_addtl_feat.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "for column in ('MarkDown%s' % i for i in range(1,6)):\n",
    "    X_addtl_feat[column].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill `CPI` and `Unemployment` with column median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_addtl_feat['CPI'].fillna(value = X_addtl_feat.CPI.median(),inplace = True)\n",
    "X_addtl_feat['Unemployment'].fillna(value = X_addtl_feat.Unemployment.median(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Joining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store and Date will be safe to join on as the formatting looks to be consistent. If the data were large it would have necessitated making the analysis I/O bound rather than memory and we would have loaded in in a database or used a chunking method with HDF5. Our simple left join would have looked something like this in SQL:\n",
    "\n",
    "```SQL\n",
    "Select *\n",
    "FROM Train_Xdf A\n",
    "LEFT JOIN addtl_features B\n",
    "ON A.Store = B.Store\n",
    "AND A.Date = B.Date\n",
    "```\n",
    "\n",
    "Join the *features* and *stores* data set with the *train* and *test* datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_traindf = pd.merge(X_traindf, X_addtl_feat, how='left', on=['Store', 'Date'])\n",
    "X_traindf = pd.merge(X_traindf, stores, how='left', on='Store')\n",
    "\n",
    "X_testdf = pd.merge(X_testdf, X_addtl_feat, how='left', on=['Store', 'Date'])\n",
    "X_testdf = pd.merge(X_testdf, stores, how='left', on='Store')\n",
    "\n",
    "# X_traindf.sort(['Store', 'Date'], ascending=[1, 2]).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Encoding\n",
    "\n",
    "_Markdown_ columns are not a result of binary encoding. The attached description does not clarify the values, but we'll have to assume it's a real scalar and not a categorical encoding based on all the unique values.\n",
    "\n",
    "The _IsHoliday_  and _Type_ columns are categorical and need to be binary-encoded, since we haven't got to NumPy yet we can utilize Pandas' *get_dummies* method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date**\n",
    "\n",
    "The Date column can be binarily encoded to create the following features:\n",
    "\n",
    "- <s>Hour of the day (24 boolean features)</s><br>\n",
    "- <s>Day of the week (7 boolean features)</s> <font color = 'red'> Only provided the first day of every week</font><br>\n",
    "- <s>Day of the month (up to 31 boolean features)</s><font color = 'red'> Ideal to specify 1-4 for week of month</font><br>\n",
    "- Month of the year (12 boolean features)<br>\n",
    "- Year (as many boolean features as they are different years in your dataset)<br>\n",
    "\n",
    "These features will enable us to identify linear dependencies on periodic events on typical time cycles. We can also create one contiunous feature derived from POSIX time.\n",
    "\n",
    "<br>\n",
    "<font color = \"dodgerblue\">Concatenate `traindf` and `testdf` before creating dummy features incase some stores/dates/departments don't exist in each others set. Split by index or 0 sales value later:</font>\n",
    "\n",
    "http://nbviewer.ipython.org/github/herrfz/dataanalysis/blob/master/assignment2/samsung_data_prediction_submitted.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_testdf[\"Weekly_Sales\"] = 0\n",
    "\n",
    "# Reorder testdf to match 'Weekly Sales' column position of traindf. \n",
    "# A better way to do this would be to pd.concat reordered slices of test together\n",
    "X_testdf = X_testdf.ix[:, ['Store','Dept','Date','Weekly_Sales','IsHoliday_x','Temperature',\n",
    "                 'Fuel_Price','MarkDown1','MarkDown2','MarkDown3',\n",
    "                 'MarkDown4','MarkDown5','CPI','Unemployment','IsHoliday_y',\n",
    "                 'Type','Size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dfDummies_concat(dfx): \n",
    "    ''' Create and concatenate named non-default dummy variables for identified columns'''\n",
    "    \n",
    "    holidayDummies = pd.get_dummies(dfx['IsHoliday_x'])\n",
    "    holidayDummies.columns = ['IsHolidayF','IsHolidayT']\n",
    "    dfx.drop('IsHoliday_y', 1, inplace=True)\n",
    "    dfx.drop('IsHoliday_x', 1, inplace=True)\n",
    "    \n",
    "    dfx['Type'] = 'Type_' + dfx['Type'].astype(str)\n",
    "    typeDummies = pd.get_dummies(dfx['Type'])\n",
    "    typeDummies.columns = ['TypeA','TypeB', 'TypeC']\n",
    "    dfx.drop('Type', 1, inplace=True)\n",
    "    \n",
    "    dfx['Store'] = 'Store_' + dfx['Store'].astype(str)\n",
    "    storeDummies = pd.get_dummies(dfx['Store'])\n",
    "    dfx.drop('Store', 1, inplace=True)\n",
    "    \n",
    "    dfx['Dept'] = 'dept_' + dfx['Dept'].astype(str)\n",
    "    deptDummies = pd.get_dummies(dfx['Dept'])\n",
    "    dfx.drop('Dept', 1, inplace=True)\n",
    "\n",
    "    # There HAS to be a faster way than this lambda f(x)\n",
    "    dateSplit = dfx['Date'].apply(lambda x: pd.Series(x.split('-')))\n",
    "    dateSplit.columns  = ['year','month','day']\n",
    "    dateSplit['year']  = 'year_' + dateSplit['year'].astype(str)\n",
    "    dateSplit['month'] = 'month_' + dateSplit['month'].astype(str)\n",
    "    dateSplit['day']   = 'day_' + dateSplit['day'].astype(str)\n",
    "    \n",
    "    dfx.drop('Date',1,inplace=True)\n",
    "    \n",
    "\n",
    "    yearDummies  = pd.get_dummies(dateSplit['year'])\n",
    "    monthDummies = pd.get_dummies(dateSplit['month'])\n",
    "    dayDummies   = pd.get_dummies(dateSplit['day'])\n",
    "\n",
    "  \n",
    "    df_concat = pd.concat( [dfx, holidayDummies, typeDummies, \n",
    "                            storeDummies, deptDummies, yearDummies,\n",
    "                            monthDummies, dayDummies],\n",
    "                          join='outer', axis=1, ignore_index=False)\n",
    "\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfConcat = pd.concat( [X_traindf, X_testdf], join = 'outer', axis=0 , ignore_index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(536634, 189)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load_ext\n",
    "# %lprun\n",
    "dfConcat = dfDummies_concat(dfConcat)\n",
    "dfConcat.shape\n",
    "\n",
    "# X_traindf = dfDummies_concat(X_traindf)\n",
    "# X_testdf = dfDummies_concat(X_testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dfConcat back into X_traindf and X_testdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(421570, 189) (115064, 189)\n"
     ]
    }
   ],
   "source": [
    "X_traindf = dfConcat[:421570]  \n",
    "X_testdf = dfConcat[421570:]  \n",
    "\n",
    "print X_traindf.shape, X_testdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create X and y NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape:(421570, 188)\n",
      "y Shape:(421570,)\n",
      "toPredict Shape: (115064, 188)\n"
     ]
    }
   ],
   "source": [
    "X = X_traindf.values[:,1:]\n",
    "y = X_traindf['Weekly_Sales'].values\n",
    "\n",
    "toPredict = X_testdf.values[:,1:] # drop empty sales col\n",
    "\n",
    "print 'X Shape:' + str(X.shape) \n",
    "print 'y Shape:' + str(y.shape)\n",
    "print 'toPredict Shape: ' + str(toPredict.shape) \n",
    "\n",
    "# import joblib\n",
    "# joblib.dump(X, 'X.pkl', compress=0, cache_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cross Validation splits\n",
    "Create train and test data splits for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape: (316177, 188)\n",
      "y_train Shape: (316177,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "print 'X_train Shape: ' + str(X_train.shape) \n",
    "print 'y_train Shape: ' + str(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Scaling\n",
    "Address zeros and negatives in `Weekly_Sales` then center all non-dummy features to the mean and component wise scale to unit variance. <font color = \"red\">Do not scale dummies! </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 315234, -1: 943})\n"
     ]
    }
   ],
   "source": [
    "# BEFORE\n",
    "# np.bincount(np.sign(y_train).astype(int))\n",
    "from collections import Counter\n",
    "print Counter(np.sign(y_train).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be the same as scaling before creating the train and test splits since a fit on X_train and y_train are being used to transform other splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# XScaler = StandardScaler().fit(X[:,0:10]) # Try fitting og X instead X_train\n",
    "XTrainScaler = StandardScaler().fit(X_train[:,0:10]) \n",
    "yScaler = StandardScaler().fit(y_train) \n",
    "# toPredictScaler = StandardScaler().fit(toPredict[:,0:10]) \n",
    "\n",
    "# Scale Train w/ X_Train\n",
    "\n",
    "X_train[:, 0:10] = XTrainScaler.transform(X_train[:,0:10])\n",
    "# y_train = yScaler.transform(y_train)\n",
    "# y_train = np.log(y_train + 1 - y_train.min())\n",
    "y_train =  log1p(y_train- y_train.min()) \n",
    "\n",
    "# Scale Test w/ X_Train\n",
    "\n",
    "X_test[:,0:10] = XTrainScaler.transform(X_test[:,0:10])\n",
    "# y_test = yScaler.transform(y_test)\n",
    "# y_test =  np.log(y_test + 1 - y_test.min())  # Should it be y_trains min and not y_test min? Not the case with X.\n",
    "y_test =  log1p(y_test - y_test.min()) \n",
    "\n",
    "# toPredict does not come from the same X as X_train & X_test do, is it logical to transform with XTrainScaler?\n",
    "# toPredict[:,0:10 ]= XTrainScaler.transform(toPredict[:,0:10])\n",
    "# toPredict[:,0:10 ]= XScaler.transform(toPredict[:,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 316176, 0: 1})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AFTER\n",
    "Counter(np.sign(y_train).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([300132]),),\n",
       " array([  9.57061353,   9.72214012,   8.56073724,  10.11692212,\n",
       "         11.22121194,  10.52838614,  10.8235135 ,   8.55351355,\n",
       "         11.22151846,   9.35209685,   9.3580377 ,   9.88105583,\n",
       "          9.48319405,   8.60317637,   8.52415689,   8.58802624,\n",
       "          8.54164187,   9.85464914,   9.41182955,   9.45255059,\n",
       "          9.05563554,   8.86294651,  10.65869926,   8.51577419,\n",
       "          8.51616066,   9.17038363,  11.05870274,   9.21214175,\n",
       "         10.5360383 ,   9.50304207,  11.080059  ,   9.43457293,\n",
       "          0.        ,   9.57213429,   9.61281769,   9.37953194,\n",
       "         10.10675977,   9.79129822,   8.56249155,   9.74193169,\n",
       "          9.29717382,  11.13505636,   9.20380808,   8.8645333 ,\n",
       "         10.08356075,   8.81884326,   8.88780181,   9.2350603 ,\n",
       "          8.85075262,  10.20708456]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y_train==0), y_train[300100:300150 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Random Forests\n",
    "<font color=\"red\">Severe bottleneck!</font>\n",
    "Olivier Grisel's joblib is a nice sklearn drop-in as RandomForest is trivially parallelizable since each processor can generate forests independently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# rf = RandomForestRegressor(n_jobs= -1)\n",
    "\n",
    "# # (n_estimators=10, criterion='mse', max_depth=None,\n",
    "# #                            min_samples_split=2, min_samples_leaf=1, max_features='auto',\n",
    "# #                            bootstrap=True, oob_score=False, n_jobs=-1, random_state=None,\n",
    "# #                            verbose=0, min_density=None, compute_importances=None)\n",
    "\n",
    "# rf.fit(X_train,y_train)\n",
    "\n",
    "# import joblib\n",
    "# joblib.dump(rf, 'rf_pickle/rf.pkl', compress=0, cache_size=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD\n",
    "\n",
    "Well start with a lineal model such as SGDRegressor. This regressor attempts to find a hyperplane that minimizes a certain loss function (the sum of squared distances from each instance to the hyperplane) using Stochastic Gradient Descent to find the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.955209</td>\n",
       "      <td> 1.489699</td>\n",
       "      <td> 0.470602</td>\n",
       "      <td> 0.661288</td>\n",
       "      <td>-0.083006</td>\n",
       "      <td>-0.075413</td>\n",
       "      <td>-0.146441</td>\n",
       "      <td> 1.100887</td>\n",
       "      <td>-0.442331</td>\n",
       "      <td>-0.139616</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.955209  1.489699  0.470602  0.661288 -0.083006 -0.075413 -0.146441   \n",
       "\n",
       "         7         8         9   10  11  12  13  14  15  16  17  18  19      \n",
       "0  1.100887 -0.442331 -0.139616   1   0   0   1   0   0   0   0   0   0 ...  \n",
       "\n",
       "[1 rows x 188 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check is features have been scaled appropriately\n",
    "pd.DataFrame(X_train[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation set is used for model selection, the test set for final model (the model which was selected by selection process) prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def trainEval(clf, X_train, y_train):    \n",
    "    clf.fit(X_train, y_train)\n",
    "    print \"Coefficient of determination on training set:\", clf.score(X_train, y_train)\n",
    "    scores = cross_val_score(clf, X_train, y_train, n_jobs =-1) #cv=cv\n",
    "    print \"Average coefficient of determination using 3-fold crossvalidation:\",np.mean(scores)\n",
    "    test_scores = cross_val_score(clf, X_test, y_test, n_jobs =-1) #cv=cv\n",
    "    print \"Score clf on X_test/y_test:\", np.mean(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.769838314209\n",
      "Average coefficient of determination using 3-fold crossvalidation: 0.768144896161\n",
      "Score clf on X_test/y_test: 0.748322735946\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "clf_sgd = linear_model.SGDRegressor(verbose=0,loss='squared_loss', penalty=None, shuffle=True, random_state=None)\n",
    "trainEval(clf_sgd,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.769050788749\n",
      "Average coefficient of determination using 3-fold crossvalidation: 0.767874629816\n",
      "Score clf on X_test/y_test: 0.748130106685\n"
     ]
    }
   ],
   "source": [
    "clf_sgd1 = linear_model.SGDRegressor(loss='squared_loss', penalty='l2', shuffle=True, random_state=None)\n",
    "trainEval(clf_sgd1,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.769124036479\n",
      "Average coefficient of determination using 3-fold crossvalidation: 0.767565505272\n",
      "Score clf on X_test/y_test: 0.747832060756\n"
     ]
    }
   ],
   "source": [
    "clf_sgd2 = linear_model.SGDRegressor(loss='squared_loss', penalty='l1', shuffle=True, random_state=None)\n",
    "trainEval(clf_sgd2,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.769313174026\n",
      "Average coefficient of determination using 3-fold crossvalidation: 0.767367899591\n",
      "Score clf on X_test/y_test: 0.747641425828\n"
     ]
    }
   ],
   "source": [
    "clf_sgd3 = linear_model.SGDRegressor(loss='squared_loss', penalty='elasticnet', shuffle=True, random_state=None)\n",
    "trainEval(clf_sgd3,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.772695558119\n",
      "Average coefficient of determination using 3-fold crossvalidation: 0.772388495731\n",
      "Score clf on X_test/y_test: 0.765669101638\n"
     ]
    }
   ],
   "source": [
    "clf_ridge = linear_model.Ridge()\n",
    "trainEval(clf_ridge,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using X_scaler:\n",
    "clf_ridge: 0.665545556463\n",
    "\n",
    "Using y_train for both y log transforms:\n",
    "clf_ridge: 0.752506048458 \n",
    "\n",
    "Using y_train and y_test for respective log transforms:\n",
    "clf_ridge:  0.768398420012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination on training set: 0.763939276859\n",
      "Average coefficient of determination using 3-fold crossvalidation: 0.73263523501\n",
      "Score clf on X_test/y_test: 0.76200253459\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import svm\n",
    "# clf_svr = svm.SVR()\n",
    "# trainEval(clf_svr,X_train[0:20000],y_train[0:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from sklearn import ensemble\n",
    "# clf_et=ensemble.ExtraTreesRegressor(n_estimators=10, n_jobs = -1)\n",
    "# trainEval(clf_et,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print sort(zip(clf_et.feature_importances_,list(X_traindf.columns)),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from scipy.stats import randint as sp_randint\n",
    "# from time import time\n",
    "# from operator import itemgetter\n",
    "# from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "# # Util function to report best scores\n",
    "# def grscoreReport(grid_scores, n_top=3):\n",
    "#     top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "#     for i, score in enumerate(top_scores):\n",
    "#         print(\"Model with rank: {0}\".format(i + 1))\n",
    "#         print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "#               score.mean_validation_score,\n",
    "#               np.std(score.cv_validation_scores)))\n",
    "#         print(\"Parameters: {0}\".format(score.parameters))\n",
    "#         print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # specify parameters and distributions to sample from\n",
    "# param_dist = {\"max_depth\": [3, None],\n",
    "#               \"max_features\": [sp_randint(1, 11),'auto'],\n",
    "#               \"min_samples_split\": sp_randint(1, 11),\n",
    "#               \"min_samples_leaf\": sp_randint(1, 11),\n",
    "#               \"bootstrap\": [True, False],\n",
    "#               \"criterion\": ['mse']}\n",
    "\n",
    "# # run randomized search\n",
    "# start = time()\n",
    "\n",
    "# n_iter_search = 10\n",
    "\n",
    "# random_search = RandomizedSearchCV(rf, param_distributions=param_dist, \n",
    "#                                    n_iter=n_iter_search, n_jobs = -1)  \n",
    "\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"RandomizedSearchCV took %.2f minutes for %d candidates\"\n",
    "#       \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "\n",
    "# grscoreReport(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.dummy import DummyRegressor \n",
    "# reg = DummyRegressor()\n",
    "# reg.fit(X_train, y_train)\n",
    "# print reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to drop the 'Weekly Sales' zeros column from X_testdf before training for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115064, 4)\n",
      "With sales columns: (421570, 189)(115064, 189)\n",
      "Without: (316177, 188)(105393, 188)\n",
      "toPredict (test.csv): (115064, 188)\n"
     ]
    }
   ],
   "source": [
    "test_csv = pd.read_table('data/test.csv', sep=',', warn_bad_lines=True, error_bad_lines=True) \n",
    "\n",
    "print test_csv.shape \n",
    "print 'With sales columns: ' + str(X_traindf.shape) + str(X_testdf.shape)\n",
    "print 'Without: ' + str(X_train.shape) + str(X_test.shape)\n",
    "print 'toPredict (test.csv): ' + str(toPredict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.19 s, sys: 15.8 ms, total: 1.2 s\n",
      "Wall time: 834 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def CurrHr():\n",
    "    epoch_time = int(time.time())\n",
    "    epoch_time = epoch_time # Round to hour\n",
    "    current_hour = datetime.datetime.fromtimestamp(epoch_time).strftime('%Y%m%d%H%M%S')\n",
    "    return current_hour\n",
    "\n",
    "\n",
    "subResults = pd.DataFrame(clf_ridge.predict(toPredict), columns = ['Weekly_Sales'])\n",
    "# subResults =  (exp(subResults)-1)\n",
    "\n",
    "subLabels = pd.DataFrame(test_csv.Store.astype(str) + '_' + \n",
    "             test_csv.Dept.astype(str) + '_' +\n",
    "             test_csv.Date.astype(str),columns = ['Id'])\n",
    "subResults.shape , subLabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Weekly_Sales\n",
      "0      0.813753\n",
      "1      1.227697\n",
      "2      0.625241\n",
      "3      0.970099\n",
      "4      0.819807\n",
      "\n",
      "[5 rows x 1 columns]\n",
      "   Weekly_Sales\n",
      "0      0.595398\n",
      "1      0.800968\n",
      "2      0.485656\n",
      "3      0.678084\n",
      "4      0.598730\n",
      "\n",
      "[5 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print (exp(subResults)-1)[:5]\n",
    "print subResults[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dropping column names for unknown reason on concat\n",
    "dfForCSV = pd.concat([subLabels,subResults], axis=1, ignore_index=True,)\n",
    "dfForCSV.columns = ['Id', 'Weekly_Sales']\n",
    "dfForCSV.to_csv(CurrHr() + '.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from ggplot import *\n",
    "# ggplot(diamonds, aes('carat', 'price')) + \\\n",
    "# geom_point(alpha=1/20.) + \\\n",
    "# ylim(0, 20000)\n",
    "\n",
    "\n",
    "# ggplot(X_traindf, aes(x=Month, y=Monthly_Sales)) + geom_bar(stat=\"identity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
